#!/usr/bin/env python3
"""
Startup script for OCR Translator Backend
Tá»± Ä‘á»™ng kiá»ƒm tra dependencies vÃ  khá»Ÿi Ä‘á»™ng server
"""

import sys
import subprocess
import os
import platform
import torch
from pathlib import Path

def check_python_version():
    """Kiá»ƒm tra Python version"""
    if sys.version_info < (3, 8):
        print("âŒ Python 3.8+ is required")
        print(f"Current version: {sys.version}")
        return False
    print(f"âœ… Python {sys.version.split()[0]}")
    return True

def check_cuda():
    """Kiá»ƒm tra CUDA availability"""
    try:
        import torch
        cuda_available = torch.cuda.is_available()
        
        if cuda_available:
            gpu_count = torch.cuda.device_count()
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            
            print(f"âœ… CUDA available")
            print(f"   GPU: {gpu_name}")
            print(f"   VRAM: {gpu_memory:.1f}GB")
            
            if gpu_memory < 3.0:
                print("âš ï¸  Warning: Limited VRAM detected (<3GB)")
                print("   Will use optimized settings")
                
        else:
            print("âš ï¸  CUDA not available - will use CPU mode")
            print("   Performance will be significantly slower")
            
        return cuda_available
        
    except ImportError:
        print("âŒ PyTorch not installed")
        return False

def check_dependencies():
    """Kiá»ƒm tra required packages"""
    package_map = {
        'fastapi': 'fastapi',
        'uvicorn': 'uvicorn',
        'torch': 'torch',
        'transformers': 'transformers',
        'paddleocr': 'paddleocr',
        'paddlepaddle-gpu': 'paddle',   # khÃ¡c tÃªn
        'opencv-python': 'cv2',         # khÃ¡c tÃªn
        'Pillow': 'PIL',                # khÃ¡c tÃªn
        'numpy': 'numpy'
    }
    
    missing = []
    
    for package, module in package_map.items():
        try:
            __import__(module)
            print(f"âœ… {package}")
        except ImportError:
            missing.append(package)
            print(f"âŒ {package}")
    
    if missing:
        print(f"\nâŒ Missing packages: {', '.join(missing)}")
        print("Install with: pip install -r requirements.txt")
        return False
    
    return True

def check_models():
    """Kiá»ƒm tra model files"""
    print("\nðŸ“¦ Checking translation models...")
    
    try:
        from transformers import M2M100Tokenizer
        
        model_name = "facebook/m2m100_418M"
        print(f"   Loading {model_name}...")
        
        # This will download if not cached
        tokenizer = M2M100Tokenizer.from_pretrained(model_name)
        print(f"âœ… Model ready: {model_name}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Model loading failed: {e}")
        print("   Models will be downloaded on first run")
        return False

def create_directories():
    """Táº¡o cÃ¡c thÆ° má»¥c cáº§n thiáº¿t"""
    dirs = ['logs', 'cache', 'debug_images']
    
    for dirname in dirs:
        Path(dirname).mkdir(exist_ok=True)
        
    print("âœ… Directories created")

def get_system_info():
    """Hiá»ƒn thá»‹ system info"""
    print("\nðŸ’» System Information:")
    print(f"   OS: {platform.system()} {platform.release()}")
    print(f"   Architecture: {platform.machine()}")
    print(f"   CPU cores: {os.cpu_count()}")
    
    # Memory info (approximate)
    try:
        import psutil
        memory = psutil.virtual_memory()
        print(f"   RAM: {memory.total / 1e9:.1f}GB ({memory.percent}% used)")
    except ImportError:
        print("   RAM: Unable to detect (install psutil for details)")

def optimize_for_rtx3050():
    """Táº¡o config file tá»‘i Æ°u cho RTX3050"""
    config_content = '''# Optimized config for RTX3050 + i5-11400H + 16GB RAM
import torch

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

HARDWARE_CONFIG = {
    "max_workers": 2,
    "batch_size": 4,
    "cache_size": 400,  # Reduced for RTX3050
    "model_precision": "fp16",  # Half precision
}

# Use smaller model for RTX3050
CURRENT_MODEL = {
    "name": "facebook/m2m100_418M",
    "type": "m2m100",
}

# OCR optimized for RTX3050
OCR_CONFIG = {
    "use_gpu": True,
    "gpu_mem": 1500,  # Limit GPU memory for OCR
    "use_textline_orientation": True,
    "lang": "en",
    "show_log": False,
    "use_space_char": True,
    "det_db_thresh": 0.3,
    "det_db_box_thresh": 0.6,
    "det_db_unclip_ratio": 1.5,
    "rec_batch_num": 4,  # Reduced batch size
    "max_text_length": 25,
    "drop_score": 0.5,
}

TRANSLATION_CONFIG = {
    "max_length": 512,
    "num_beams": 2,
    "early_stopping": True,
    "do_sample": False,
    "source_lang": "en",
    "target_lang": "vi",
    "paragraph_distance": 50,
}

API_CONFIG = {
    "host": "127.0.0.1",
    "port": 8000,
    "reload": False,
    "workers": 1,
}
'''
    
    if not os.path.exists('config.py'):
        with open('config.py', 'w') as f:
            f.write(config_content)
        print("âœ… Created optimized config.py for RTX3050")
    else:
        print("âœ… Config file exists")

def start_server():
    """Khá»Ÿi Ä‘á»™ng server"""
    print("\nðŸš€ Starting OCR Translator Server...")
    print("="*50)
    
    try:
        # Sá»­ dá»¥ng main_optimized.py náº¿u cÃ³, ngÆ°á»£c láº¡i dÃ¹ng main.py
        if os.path.exists('main_optimized.py'):
            cmd = [sys.executable, 'main_optimized.py']
            print("Using optimized main file")
        else:
            cmd = [sys.executable, 'main.py']
            print("Using standard main file")
            
        # Run server
        subprocess.run(cmd, check=True)
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  Server stopped by user")
    except subprocess.CalledProcessError as e:
        print(f"\nâŒ Server failed to start: {e}")
        return False
    except FileNotFoundError:
        print("âŒ Main file not found!")
        return False
    
    return True

def main():
    """Main startup sequence"""
    print("ðŸ”§ OCR Translator - Startup Check")
    print("="*50)
    
    # System checks
    if not check_python_version():
        sys.exit(1)
    
    get_system_info()
    
    cuda_available = check_cuda()
    
    print("\nðŸ“‹ Checking dependencies...")
    if not check_dependencies():
        print("\nâŒ Please install missing dependencies first:")
        print("pip install -r requirements.txt")
        sys.exit(1)
    
    # Setup
    print("\nâš™ï¸  Setting up...")
    create_directories()
    
    if cuda_available:
        optimize_for_rtx3050()
    
    # Model check (optional)
    check_models()
    
    # Performance tips
    if cuda_available:
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        if gpu_memory <= 4.5:  # RTX3050 range
            print("\nðŸ’¡ RTX3050 detected - Performance tips:")
            print("   â€¢ Close other GPU applications")
            print("   â€¢ Use smaller batch sizes if OOM occurs")
            print("   â€¢ Monitor GPU memory usage")
    else:
        print("\nðŸ’¡ CPU mode - Performance tips:")
        print("   â€¢ Close unnecessary applications")
        print("   â€¢ Consider using smaller images")
        print("   â€¢ Translation will be slower but functional")
    
    print("\nâœ… All checks passed!")
    
    # Start server
    input("\nPress Enter to start the server (or Ctrl+C to exit)...")
    start_server()

if __name__ == "__main__":
    main()